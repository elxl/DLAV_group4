{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipeNote: you may need to restart the kernel to use updated packages.\n",
      "  Using cached mediapipe-0.8.10-cp39-cp39-win_amd64.whl (48.6 MB)\n",
      "Collecting opencv-contrib-python\n",
      "\n",
      "  Using cached opencv_contrib_python-4.5.5.64-cp36-abi3-win_amd64.whl (42.2 MB)\n",
      "Collecting attrs>=19.1.0\n",
      "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
      "Collecting protobuf>=3.11.4\n",
      "  Using cached protobuf-3.20.1-cp39-cp39-win_amd64.whl (904 kB)\n",
      "Collecting absl-py\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\11481\\anaconda3\\envs\\pytorch\\lib\\site-packages (from mediapipe) (1.21.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\11481\\anaconda3\\envs\\pytorch\\lib\\site-packages (from mediapipe) (3.5.1)\n",
      "Requirement already satisfied: six in c:\\users\\11481\\anaconda3\\envs\\pytorch\\lib\\site-packages (from absl-py->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\11481\\anaconda3\\envs\\pytorch\\lib\\site-packages (from matplotlib->mediapipe) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\11481\\anaconda3\\envs\\pytorch\\lib\\site-packages (from matplotlib->mediapipe) (3.0.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\11481\\anaconda3\\envs\\pytorch\\lib\\site-packages (from matplotlib->mediapipe) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\11481\\anaconda3\\envs\\pytorch\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\11481\\anaconda3\\envs\\pytorch\\lib\\site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\11481\\anaconda3\\envs\\pytorch\\lib\\site-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\11481\\anaconda3\\envs\\pytorch\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Installing collected packages: protobuf, opencv-contrib-python, attrs, absl-py, mediapipe\n",
      "Successfully installed absl-py-1.0.0 attrs-21.4.0 mediapipe-0.8.10 opencv-contrib-python-4.5.5.64 protobuf-3.20.1\n"
     ]
    }
   ],
   "source": [
    "%pip install mediapipe\n",
    "%conda install pandas\n",
    "%conda install requests\n",
    "%pip install pyyaml\n",
    "%pip install tqdm\n",
    "%pip install seaborn\n",
    "%conda install -c conda-forge tensorflow\n",
    "%conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "%conda install -c conda-forge easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov4-deepsort'...\n"
     ]
    }
   ],
   "source": [
    "# clone repository for deepsort with yolov4\n",
    "!git clone https://github.com/theAIGuysCode/yolov4-deepsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\11481\\Desktop\\DLAV\\yolov4-deepsort\n"
     ]
    }
   ],
   "source": [
    "# step into the yolov4-deepsort folder\n",
    "%cd yolov4-deepsort/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\11481/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2022-5-16 Python-3.7.13 torch-1.11.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# load detection model\n",
    "import torch\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "model.confidence = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from IPython.display import display, Javascript, Image\n",
    "from base64 import b64decode, b64encode\n",
    "import cv2\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import io\n",
    "import html\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
    "def bbox_to_bytes(bbox_array):\n",
    "  \"\"\"\n",
    "  Params:\n",
    "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
    "  Returns:\n",
    "        bytes: Base64 image byte string\n",
    "  \"\"\"\n",
    "  # convert array into PIL image\n",
    "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
    "  iobuf = io.BytesIO()\n",
    "  # format bbox into png for return\n",
    "  bbox_PIL.save(iobuf, format='png')\n",
    "  # format return string\n",
    "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
    "\n",
    "  return bbox_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\11481\\anaconda3\\envs\\tf-torch\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "from re import I\n",
    "import os\n",
    "# comment out below line to enable tensorflow logging outputs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import time\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "import core.utils as utils\n",
    "from core.yolov4 import filter_boxes\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from absl.flags import FLAGS\n",
    "from core.config import cfg\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "# deep sort imports\n",
    "from deep_sort import preprocessing, nn_matching\n",
    "from deep_sort.detection import Detection\n",
    "from deep_sort.tracker import Tracker\n",
    "from tools import generate_detections as gdet\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_holistic = mp.solutions.holistic\n",
    "def main():\n",
    "  # Definition of the parameters\n",
    "  max_cosine_distance = 0.4\n",
    "  nn_budget = 100\n",
    "  nms_max_overlap = 1.0\n",
    "  \n",
    "  # initialize deep sort\n",
    "  model_filename = 'model_data/mars-small128.pb'\n",
    "  encoder = gdet.create_box_encoder(model_filename, batch_size=1)\n",
    "  # calculate cosine distance metric\n",
    "  metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "  # initialize tracker\n",
    "  tracker = Tracker(metric, max_age=200)\n",
    "\n",
    "  # load configuration for object detector\n",
    "  config = ConfigProto()\n",
    "  config.gpu_options.allow_growth = True\n",
    "  session = InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "  # Flags\n",
    "  flaginfo = False\n",
    "\n",
    "\n",
    "  # read in all class names from config\n",
    "  class_names = utils.read_class_names(cfg.YOLO.CLASSES)\n",
    "\n",
    "  # start streaming video from webcam\n",
    "  cap = cv2.VideoCapture(0)\n",
    "\n",
    "  # initialze bounding box to empty\n",
    "  bbox = ''\n",
    "  frame_num = 0\n",
    "  initialisation = True\n",
    "  count = 0\n",
    "  IDofInterest = -1\n",
    "  # while video is running\n",
    "  with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # convert JS response to OpenCV Image    \n",
    "        #frame = js_to_image(js_reply[\"img\"])\n",
    "\n",
    "        # grayscale image for face detection\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # create transparent overlay for bounding box\n",
    "        bbox_array = np.zeros([480,640,4], dtype=np.uint8) \n",
    "        \n",
    "        frame_num +=1\n",
    "        #print('Frame #: ', frame_num)\n",
    "        frame_size = frame.shape[:2]\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Detection\n",
    "        result = model(frame) # inference for person\n",
    "        detectPerson = result.pandas().xyxy[0] # Get the bounding box, the confidence and the class\n",
    "        detectPerson = detectPerson [(detectPerson['class']== 0)]\n",
    "        \n",
    "        if initialisation :\n",
    "          \n",
    "          num_objects = 0\n",
    "          bboxes = np.array([])\n",
    "          scores = np.array([])\n",
    "          classes = np.array([])\n",
    "          detectPersonOfInterest = pd.Series(dtype='float64')\n",
    "          \n",
    "          for i in detectPerson.index:\n",
    "            cur_person = detectPerson.iloc[i,:]\n",
    "            xmin = int(cur_person['xmin'])\n",
    "            xmax = int(cur_person['xmax'])\n",
    "            ymin = int(cur_person['ymin'])\n",
    "            ymax = int(cur_person['ymax'])\n",
    "            frame_crop = np.ascontiguousarray(frame[ymin:(ymax+1),xmin:(xmax+1),:])\n",
    "\n",
    "            frame_crop.flags.writeable = False \n",
    "\n",
    "            # Make detection\n",
    "            results = pose.process(frame_crop)\n",
    "            frame_crop.flags.writeable = True\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "              # Extract landmarks \n",
    "              landmarks = results.pose_landmarks.landmark\n",
    "              left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "              right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "              left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
    "              right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
    "\n",
    "    \n",
    "\n",
    "              if (left_wrist.y < left_shoulder.y) and (right_wrist.y < right_shoulder.y):\n",
    "                detectPersonOfInterest = detectPerson.iloc[i,:]         \n",
    "                bbox_array = cv2.rectangle(bbox_array,(int(detectPersonOfInterest['xmin']),int(detectPersonOfInterest['ymin'])),(int(detectPersonOfInterest['xmax']),int(detectPersonOfInterest['ymax'])),(255,0,0),2)\n",
    "                bbox_array = cv2.putText(bbox_array, \"{} [{:.2f}]\".format(detectPersonOfInterest['name'], float(detectPersonOfInterest['confidence'])),\n",
    "                                  (int(detectPersonOfInterest['xmin']), int(detectPersonOfInterest['ymin']) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                                  (255,0,0), 2) \n",
    "              \n",
    "\n",
    "                num_objects=1\n",
    "                bboxes = np.zeros((1,4))\n",
    "                scores = np.zeros(1)\n",
    "                classes = np.zeros(1)\n",
    "                bboxes[0,0] = int(detectPersonOfInterest['ymin'])/frame_size[0]\n",
    "                bboxes[0,1] = int(detectPersonOfInterest['xmin'])/frame_size[1]\n",
    "                bboxes[0,2] = int(detectPersonOfInterest['ymax'])/frame_size[0]\n",
    "                bboxes[0,3] = int(detectPersonOfInterest['xmax'])/frame_size[1]\n",
    "                scores[0] = np.array(float(detectPerson.iloc[0]['confidence']))\n",
    "                classes[0] = np.array(0) # class person\n",
    "                count += 1 \n",
    "                if count > 10: \n",
    "                  initialisation = False\n",
    "                break                     \n",
    "\n",
    "        elif not detectPerson.empty:\n",
    "        # convert data to numpy arrays and slice out unused elements\n",
    "          \n",
    "          num_objects = detectPerson.shape[0]\n",
    "\n",
    "          bboxes = np.zeros((num_objects,4))\n",
    "          scores = np.zeros(num_objects)\n",
    "          classes = np.zeros(num_objects)\n",
    "\n",
    "          for i in range(num_objects):\n",
    "            #print('numobjects', num_objects)\n",
    "            #print('i',i)\n",
    "            bboxes[i,0] = int(detectPerson.iloc[i]['ymin'])/frame_size[0]\n",
    "            bboxes[i,1] = int(detectPerson.iloc[i]['xmin'])/frame_size[1]\n",
    "            bboxes[i,2] = int(detectPerson.iloc[i]['ymax'])/frame_size[0]\n",
    "            bboxes[i,3] = int(detectPerson.iloc[i]['xmax'])/frame_size[1]\n",
    "            scores[i] = np.array(float(detectPerson.iloc[i]['confidence']))\n",
    "            classes[i] = np.array(0) # class person\n",
    "\n",
    "        else:\n",
    "          \n",
    "          num_objects = 0\n",
    "          bboxes = np.array([])\n",
    "          scores = np.array([])\n",
    "          classes = np.array([])\n",
    "\n",
    "        # format bounding boxes from normalized ymin, xmin, ymax, xmax ---> xmin, ymin, width, height\n",
    "        original_h, original_w, _ = frame.shape\n",
    "        bboxes = utils.format_boxes(bboxes, original_h, original_w)\n",
    "\n",
    "        # store all predictions in one parameter for simplicity when calling functions\n",
    "        pred_bbox = [bboxes, scores, classes, num_objects]\n",
    "\n",
    "        # loop through objects and use class index to get class name, allow only classes in allowed_classes list\n",
    "        names = []\n",
    "        for i in range(num_objects):\n",
    "            class_indx = int(classes[i])\n",
    "            class_name = class_names[class_indx]\n",
    "            names.append(class_name)\n",
    "\n",
    "\n",
    "        # encode yolo detections and feed to tracker\n",
    "        features = encoder(frame, bboxes)\n",
    "        detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in zip(bboxes, scores, names, features)]\n",
    "       \n",
    "        #initialize color map\n",
    "        cmap = plt.get_cmap('tab20b')\n",
    "        colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\n",
    "\n",
    "        # run non-maxima supression\n",
    "        boxs = np.array([d.tlwh for d in detections])\n",
    "        scores = np.array([d.confidence for d in detections])\n",
    "        classes = np.array([d.class_name for d in detections])\n",
    "        indices = preprocessing.non_max_suppression(boxs, classes, nms_max_overlap, scores)\n",
    "        detections = [detections[i] for i in indices]       \n",
    "        \n",
    "        # Call the tracker\n",
    "        tracker.predict()\n",
    "        tracker.update(detections)\n",
    "\n",
    "        # update tracks\n",
    "        for track in tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue \n",
    "            bbox = track.to_tlbr()\n",
    "            class_name = track.get_class()\n",
    "            if initialisation and not detectPersonOfInterest.empty:\n",
    "              IDofInterest = track.track_id\n",
    "              #print(IDofInterest)\n",
    "\n",
    "            if track.track_id == IDofInterest :\n",
    "            # draw bbox on screen\n",
    "              color = colors[int(track.track_id) % len(colors)]\n",
    "              color = [i * 255 for i in color]\n",
    "              cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2)\n",
    "              cv2.rectangle(frame, (int(bbox[0]), int(bbox[1]-30)), (int(bbox[0])+(len(class_name)+len(str(track.track_id)))*17, int(bbox[1])), color, -1)\n",
    "              cv2.putText(frame, class_name + \"-\" + str(track.track_id),(int(bbox[0]), int(bbox[1]-10)),0, 0.75, (255,255,255),2)\n",
    "\n",
    "              # get face bounding box for overlay\n",
    "              bbox_array = cv2.rectangle(bbox_array, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2)\n",
    "              bbox_array = cv2.putText(bbox_array, class_name + \"-\" + str(track.track_id),(int(bbox[0]), int(bbox[1]-10)),0, 0.75, (255,255,255),2)\n",
    "\n",
    "            # if enable info flag then print details about each track\n",
    "            if flaginfo:\n",
    "                print(\"Tracker ID: {}, Class: {},  BBox Coords (xmin, ymin, xmax, ymax): {}\".format(str(track.track_id), class_name, (int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3]))))\n",
    "\n",
    "        # calculate frames per second of running detections\n",
    "        fps = 1.0 / (time.time() - start_time)\n",
    "        #print(\"FPS: %.2f\" % fps)\n",
    "        result = np.asarray(frame)\n",
    "        result = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        \n",
    "        cv2.imshow(\"tracking\", result)\n",
    "\n",
    "        bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
    "        # convert overlay of bbox into bytes\n",
    "        bbox_bytes = bbox_to_bytes(bbox_array)\n",
    "        # update bbox so next frame gets new overlay\n",
    "        bbox = bbox_bytes\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'): break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f64ac8a1da115e514698c3a14cfcdc55d3465a052dd5b558bad1cd017a71e6b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('tf-torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
