{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Milestone2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a028ed98cafa4b6184ffc845608e16ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49280cc50be44f45b657a3f46915e283",
              "IPY_MODEL_6d1dfcc3c2e845cc95769ee0ec6af477",
              "IPY_MODEL_584a50f3711e431691d3f6067e3fef0e"
            ],
            "layout": "IPY_MODEL_a1f13b09c5b84fb2a23420b8fa892efd"
          }
        },
        "49280cc50be44f45b657a3f46915e283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_383638d9e1ac42d68c049ad7655d082a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6c59167e68314ee989c1fa6b396e9775",
            "value": "100%"
          }
        },
        "6d1dfcc3c2e845cc95769ee0ec6af477": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_138f34d3c03f4055b2bb081d5c8c1770",
            "max": 14808437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c246a7f58008463bb82d90fae8a232b0",
            "value": 14808437
          }
        },
        "584a50f3711e431691d3f6067e3fef0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f3cd8bf8ae4460e82365dc1e6182408",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_39e0b8edc416437aa56b8824d225023b",
            "value": " 14.1M/14.1M [00:00&lt;00:00, 26.2MB/s]"
          }
        },
        "a1f13b09c5b84fb2a23420b8fa892efd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "383638d9e1ac42d68c049ad7655d082a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c59167e68314ee989c1fa6b396e9775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "138f34d3c03f4055b2bb081d5c8c1770": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c246a7f58008463bb82d90fae8a232b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f3cd8bf8ae4460e82365dc1e6182408": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39e0b8edc416437aa56b8824d225023b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Download model and yolo5 weights"
      ],
      "metadata": {
        "id": "N2l2_CHoz__w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AgsqArV0-Wd",
        "outputId": "445ff9e8-e3ea-4f75-c6b3-dfedc3c9c1e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.8.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.8 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32.8 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.21.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.4->mediapipe) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.2.0)\n",
            "Installing collected packages: mediapipe\n",
            "Successfully installed mediapipe-0.8.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone repository for deepsort with yolov4\n",
        "!git clone https://github.com/theAIGuysCode/yolov4-deepsort"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YJDW9lG0G9r",
        "outputId": "ad41724f-b6a9-42f1-b52a-96cabb7ab051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov4-deepsort'...\n",
            "remote: Enumerating objects: 143, done.\u001b[K\n",
            "remote: Total 143 (delta 0), reused 0 (delta 0), pack-reused 143\u001b[K\n",
            "Receiving objects: 100% (143/143), 76.84 MiB | 37.08 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step into the yolov4-deepsort folder\n",
        "%cd yolov4-deepsort/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw3ucmTR0JYZ",
        "outputId": "f2a65c6d-8902-4fa5-a28f-5c2c2d125caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov4-deepsort\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load detection model\n",
        "import torch\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "model.confidence = 0.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445,
          "referenced_widgets": [
            "a028ed98cafa4b6184ffc845608e16ad",
            "49280cc50be44f45b657a3f46915e283",
            "6d1dfcc3c2e845cc95769ee0ec6af477",
            "584a50f3711e431691d3f6067e3fef0e",
            "a1f13b09c5b84fb2a23420b8fa892efd",
            "383638d9e1ac42d68c049ad7655d082a",
            "6c59167e68314ee989c1fa6b396e9775",
            "138f34d3c03f4055b2bb081d5c8c1770",
            "c246a7f58008463bb82d90fae8a232b0",
            "9f3cd8bf8ae4460e82365dc1e6182408",
            "39e0b8edc416437aa56b8824d225023b"
          ]
        },
        "id": "x-5Pb3-4QOVa",
        "outputId": "e474e54d-1613-46a8-c74d-8e42780f1716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m PyYAML>=5.3.1 not found and is required by YOLOv5, attempting auto-update...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyYAML>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "Installing collected packages: PyYAML\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "YOLOv5 üöÄ 2022-6-1 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5s.pt...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/14.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a028ed98cafa4b6184ffc845608e16ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run tracker on webcam"
      ],
      "metadata": {
        "id": "mTWDkQVL0TMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "TrFS_O0x0U8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "metadata": {
        "id": "Sa1j8X3f0Wkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "QjsIib0i0aaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import I\n",
        "import os\n",
        "# comment out below line to enable tensorflow logging outputs\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import time\n",
        "import tensorflow as tf\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "import core.utils as utils\n",
        "from core.yolov4 import filter_boxes\n",
        "from tensorflow.python.saved_model import tag_constants\n",
        "from absl.flags import FLAGS\n",
        "from core.config import cfg\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "# deep sort imports\n",
        "from deep_sort import preprocessing, nn_matching\n",
        "from deep_sort.detection import Detection\n",
        "from deep_sort.tracker import Tracker\n",
        "from tools import generate_detections as gdet\n",
        "import pandas as pd\n",
        "import mediapipe as mp\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_pose = mp.solutions.pose\n",
        "\n",
        "def main():\n",
        "  # Definition of the parameters\n",
        "  max_cosine_distance = 0.05\n",
        "  nn_budget = 100\n",
        "  nms_max_overlap = 1.0\n",
        "  \n",
        "  # initialize deep sort\n",
        "  model_filename = 'model_data/mars-small128.pb'\n",
        "  encoder = gdet.create_box_encoder(model_filename, batch_size=1)\n",
        "  # calculate cosine distance metric\n",
        "  metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
        "  # initialize tracker\n",
        "  tracker = Tracker(metric, max_age=200)\n",
        "\n",
        "  # load configuration for object detector\n",
        "  config = ConfigProto()\n",
        "  config.gpu_options.allow_growth = True\n",
        "  session = InteractiveSession(config=config)\n",
        "\n",
        "\n",
        "  # Flags\n",
        "  input_size = 416\n",
        "  flagtiny = False\n",
        "  flagiou = 0.45\n",
        "  flagscore = 0.5\n",
        "  flagcount = False\n",
        "  flaginfo = False\n",
        "  flagdont_show = False\n",
        "\n",
        "  # read in all class names from config\n",
        "  class_names = utils.read_class_names(cfg.YOLO.CLASSES)\n",
        "\n",
        "  # start streaming video from webcam\n",
        "  video_stream()\n",
        "  # label for video\n",
        "  label_html = 'Capturing...'\n",
        "  # initialze bounding box to empty\n",
        "  bbox = ''\n",
        "  frame_num = 0\n",
        "  initialisation = True\n",
        "  count = 0\n",
        "  IDofInterest = -1\n",
        "  # while video is running\n",
        "  with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
        "    while True:\n",
        "        js_reply = video_frame(label_html, bbox)\n",
        "        if not js_reply:\n",
        "            break\n",
        "\n",
        "        # convert JS response to OpenCV Image    \n",
        "        frame = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "        # grayscale image for face detection\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # create transparent overlay for bounding box\n",
        "        bbox_array = np.zeros([480,640,4], dtype=np.uint8) \n",
        "        \n",
        "        frame_num +=1\n",
        "        #print('Frame #: ', frame_num)\n",
        "        frame_size = frame.shape[:2]\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Detection\n",
        "        result = model(frame) # inference for person\n",
        "        detectPerson = result.pandas().xyxy[0] # Get the bounding box, the confidence and the class\n",
        "        detectPerson = detectPerson [(detectPerson['class']== 0)]\n",
        "        \n",
        "        if initialisation :\n",
        "          \n",
        "          num_objects = 0\n",
        "          bboxes = np.array([])\n",
        "          scores = np.array([])\n",
        "          classes = np.array([])\n",
        "          detectPersonOfInterest = pd.Series(dtype='float64')\n",
        "\n",
        "          for i in detectPerson.index:\n",
        "            cur_person = detectPerson.iloc[i,:]\n",
        "            xmin = int(cur_person['xmin'])\n",
        "            xmax = int(cur_person['xmax'])\n",
        "            ymin = int(cur_person['ymin'])\n",
        "            ymax = int(cur_person['ymax'])\n",
        "            frame_crop = np.ascontiguousarray(frame[ymin:(ymax+1),xmin:(xmax+1),:])\n",
        "\n",
        "            frame_crop.flags.writeable = False \n",
        "\n",
        "            # Make detection\n",
        "            results = pose.process(frame_crop)\n",
        "            frame_crop.flags.writeable = True\n",
        "\n",
        "            # Extract landmarks \n",
        "            if not results:\n",
        "              continue\n",
        "            if not results.pose_landmarks:\n",
        "              continue\n",
        "            landmarks = results.pose_landmarks.landmark\n",
        "            left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
        "            right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
        "            left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
        "            right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
        "\n",
        "  \n",
        "\n",
        "            if (left_wrist.y < left_shoulder.y) and (right_wrist.y < right_shoulder.y):\n",
        "              detectPersonOfInterest = detectPerson.iloc[i,:]         \n",
        "              bbox_array = cv2.rectangle(bbox_array,(int(detectPersonOfInterest['xmin']),int(detectPersonOfInterest['ymin'])),(int(detectPersonOfInterest['xmax']),int(detectPersonOfInterest['ymax'])),(255,0,0),2)\n",
        "              bbox_array = cv2.putText(bbox_array, \"{} [{:.2f}]\".format(detectPersonOfInterest['name'], float(detectPersonOfInterest['confidence'])),\n",
        "                                (int(detectPersonOfInterest['xmin']), int(detectPersonOfInterest['ymin']) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                                (255,0,0), 2) \n",
        "            \n",
        "\n",
        "              num_objects=1\n",
        "              bboxes = np.zeros((1,4))\n",
        "              scores = np.zeros(1)\n",
        "              classes = np.zeros(1)\n",
        "              bboxes[0,0] = int(detectPersonOfInterest['ymin'])/frame_size[0]\n",
        "              bboxes[0,1] = int(detectPersonOfInterest['xmin'])/frame_size[1]\n",
        "              bboxes[0,2] = int(detectPersonOfInterest['ymax'])/frame_size[0]\n",
        "              bboxes[0,3] = int(detectPersonOfInterest['xmax'])/frame_size[1]\n",
        "              scores[0] = np.array(float(detectPerson.iloc[0]['confidence']))\n",
        "              classes[0] = np.array(0) # class person\n",
        "              count += 1 \n",
        "              if count > 10: \n",
        "                initialisation = False\n",
        "              break                     \n",
        "\n",
        "        elif not detectPerson.empty:\n",
        "        # convert data to numpy arrays and slice out unused elements\n",
        "          \n",
        "          num_objects = detectPerson.shape[0]\n",
        "\n",
        "          bboxes = np.zeros((num_objects,4))\n",
        "          scores = np.zeros(num_objects)\n",
        "          classes = np.zeros(num_objects)\n",
        "\n",
        "          for i in range(num_objects):\n",
        "            #print('numobjects', num_objects)\n",
        "            #print('i',i)\n",
        "            bboxes[i,0] = int(detectPerson.iloc[i]['ymin'])/frame_size[0]\n",
        "            bboxes[i,1] = int(detectPerson.iloc[i]['xmin'])/frame_size[1]\n",
        "            bboxes[i,2] = int(detectPerson.iloc[i]['ymax'])/frame_size[0]\n",
        "            bboxes[i,3] = int(detectPerson.iloc[i]['xmax'])/frame_size[1]\n",
        "            scores[i] = np.array(float(detectPerson.iloc[i]['confidence']))\n",
        "            classes[i] = np.array(0) # class person\n",
        "\n",
        "        else:\n",
        "          \n",
        "          num_objects = 0\n",
        "          bboxes = np.array([])\n",
        "          scores = np.array([])\n",
        "          classes = np.array([])\n",
        "\n",
        "        # format bounding boxes from normalized ymin, xmin, ymax, xmax ---> xmin, ymin, width, height\n",
        "        original_h, original_w, _ = frame.shape\n",
        "        bboxes = utils.format_boxes(bboxes, original_h, original_w)\n",
        "\n",
        "        # store all predictions in one parameter for simplicity when calling functions\n",
        "        pred_bbox = [bboxes, scores, classes, num_objects]\n",
        "\n",
        "        # loop through objects and use class index to get class name, allow only classes in allowed_classes list\n",
        "        names = []\n",
        "        for i in range(num_objects):\n",
        "            class_indx = int(classes[i])\n",
        "            class_name = class_names[class_indx]\n",
        "            names.append(class_name)\n",
        "\n",
        "\n",
        "        # encode yolo detections and feed to tracker\n",
        "        features = encoder(frame, bboxes)\n",
        "        detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in zip(bboxes, scores, names, features)]\n",
        "       \n",
        "        #initialize color map\n",
        "        cmap = plt.get_cmap('tab20b')\n",
        "        colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\n",
        "\n",
        "        # run non-maxima supression\n",
        "        boxs = np.array([d.tlwh for d in detections])\n",
        "        scores = np.array([d.confidence for d in detections])\n",
        "        classes = np.array([d.class_name for d in detections])\n",
        "        indices = preprocessing.non_max_suppression(boxs, classes, nms_max_overlap, scores)\n",
        "        detections = [detections[i] for i in indices]       \n",
        "        \n",
        "        # Call the tracker\n",
        "        tracker.predict()\n",
        "        tracker.update(detections)\n",
        "\n",
        "        # update tracks\n",
        "        for track in tracker.tracks:\n",
        "            if not track.is_confirmed() or track.time_since_update > 1:\n",
        "                continue \n",
        "            bbox = track.to_tlbr()\n",
        "            class_name = track.get_class()\n",
        "            if initialisation and not detectPersonOfInterest.empty:\n",
        "              IDofInterest = track.track_id\n",
        "              #print(IDofInterest)\n",
        "\n",
        "            if track.track_id == IDofInterest :\n",
        "            # draw bbox on screen\n",
        "              color = colors[int(track.track_id) % len(colors)]\n",
        "              color = [i * 255 for i in color]\n",
        "              cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2)\n",
        "              cv2.rectangle(frame, (int(bbox[0]), int(bbox[1]-30)), (int(bbox[0])+(len(class_name)+len(str(track.track_id)))*17, int(bbox[1])), color, -1)\n",
        "              cv2.putText(frame, class_name + \"-\" + str(track.track_id),(int(bbox[0]), int(bbox[1]-10)),0, 0.75, (255,255,255),2)\n",
        "\n",
        "              # get face bounding box for overlay\n",
        "              bbox_array = cv2.rectangle(bbox_array, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2)\n",
        "              bbox_array = cv2.putText(bbox_array, class_name + \"-\" + str(track.track_id),(int(bbox[0]), int(bbox[1]-10)),0, 0.75, (255,255,255),2)\n",
        "\n",
        "            # if enable info flag then print details about each track\n",
        "            if flaginfo:\n",
        "                print(\"Tracker ID: {}, Class: {},  BBox Coords (xmin, ymin, xmax, ymax): {}\".format(str(track.track_id), class_name, (int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3]))))\n",
        "\n",
        "        # calculate frames per second of running detections\n",
        "        fps = 1.0 / (time.time() - start_time)\n",
        "        #print(\"FPS: %.2f\" % fps)\n",
        "        result = np.asarray(frame)\n",
        "        result = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "        \n",
        "        #if not flagdont_show:\n",
        "            #cv2_imshow(result)\n",
        "\n",
        "        bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "        # convert overlay of bbox into bytes\n",
        "        bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "        # update bbox so next frame gets new overlay\n",
        "        bbox = bbox_bytes\n",
        "\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "urEfYNl_804f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "3cd0588f-7c5a-46c8-983d-869096dfe5c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9I1vYXW3VG_e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}